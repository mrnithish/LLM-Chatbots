{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77f6ff43-e425-424c-9dae-f42a1ec7092f",
      "metadata": {
        "id": "77f6ff43-e425-424c-9dae-f42a1ec7092f",
        "tags": []
      },
      "source": [
        "## Building Conversational Bot using Langchain\n",
        "\n",
        "* In this notebook we shall utilize the capabilities of langchain (PromptTemplate, LLMChain, LLM interface </br>\n",
        "</br>\n",
        "We shall build a conversational bot and create an interface like ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101fbee4-4adc-4248-94d1-44e328e99756",
      "metadata": {
        "id": "101fbee4-4adc-4248-94d1-44e328e99756"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e1582b-ca9f-4480-a89e-76c84bf9bc50",
      "metadata": {
        "id": "b7e1582b-ca9f-4480-a89e-76c84bf9bc50",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip install huggingface_hub\n",
        "! pip install transformers\n",
        "! pip install langchain\n",
        "! pip install chainlit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6aa3f7-adad-4997-9d86-17dd9fdfc212",
      "metadata": {
        "id": "dd6aa3f7-adad-4997-9d86-17dd9fdfc212"
      },
      "source": [
        "I have already installed these libraries in my environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bb339a-be73-4a76-9412-75ef63177c26",
      "metadata": {
        "id": "d0bb339a-be73-4a76-9412-75ef63177c26",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip install chainlit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6329b396-47bc-47ce-ad86-8906cc1e2d06",
      "metadata": {
        "id": "6329b396-47bc-47ce-ad86-8906cc1e2d06"
      },
      "source": [
        "### Importing libraries and access token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a969e5-0054-4545-b26b-812923ec2f74",
      "metadata": {
        "id": "e4a969e5-0054-4545-b26b-812923ec2f74",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chainlit as cl\n",
        "from langchain import HuggingFaceHub, PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T6rQ0MhzQGUZ",
      "metadata": {
        "id": "T6rQ0MhzQGUZ"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community\n",
        "! pip install -U langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7d2430-1cea-45f3-92a9-2fe1f01cde4d",
      "metadata": {
        "id": "ef7d2430-1cea-45f3-92a9-2fe1f01cde4d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "# HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"Your API Key\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e572beee-4dcb-4093-9fd3-78ff9c075fd7",
      "metadata": {
        "id": "e572beee-4dcb-4093-9fd3-78ff9c075fd7"
      },
      "source": [
        "* The PromptTemplate is one of the elements of LangChain, necessary for building applications based on the Large Language Model. It defines how the model should interpret the userâ€™s questions and in what context it should answer them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6ef582-ecd6-47b1-87e6-1fefa1b24085",
      "metadata": {
        "id": "3b6ef582-ecd6-47b1-87e6-1fefa1b24085"
      },
      "source": [
        "### Setting the conversational model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5aefe8e-c88f-491f-8894-741b0fc74600",
      "metadata": {
        "id": "b5aefe8e-c88f-491f-8894-741b0fc74600",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#model_id = \"microsoft/DialoGPT-medium\" : conversational models are not currently supported by Langchain\n",
        "#model_id = \"mosaicml/mpt-7b-instruct\"\n",
        "#model_id = \"tiiuae/falcon-7b\"\n",
        "model_id = \"gpt2-medium\"  #355M parameters\n",
        "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
        "                            repo_id=model_id,\n",
        "                            model_kwargs={\"temperature\":0.6, \"max_new_tokens\":300}) #0 to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f4407e-dfe4-4b7e-9b9d-11ea047e3968",
      "metadata": {
        "id": "58f4407e-dfe4-4b7e-9b9d-11ea047e3968",
        "tags": []
      },
      "outputs": [],
      "source": [
        "template = \"\"\"you are a story writer AI assistant that completes a story based on the query\n",
        "received as input\n",
        "\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['query'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fa89aee-7e5f-4af3-9164-87654529f9ce",
      "metadata": {
        "id": "7fa89aee-7e5f-4af3-9164-87654529f9ce",
        "tags": []
      },
      "outputs": [],
      "source": [
        "conv_chain = LLMChain(llm=conv_model,\n",
        "                      prompt=prompt,\n",
        "                      verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782ecce6-dc8c-4859-a0b7-bf0bb05b9b8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "782ecce6-dc8c-4859-a0b7-bf0bb05b9b8c",
        "outputId": "3ddedc40-4c60-4e4e-f419-a2c2677ded49",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3myou are a story writer AI assistant that completes a story based on the query\n",
            "received as input\n",
            "\n",
            "Once upon a time in 1980 \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "you are a story writer AI assistant that completes a story based on the query\n",
            "received as input\n",
            "\n",
            "Once upon a time in 1980 \n",
            "\n",
            "A&A took its name from the A&A station in the Channel Islands. A&A is a programme that is displayed on the screen at an interval of 60 seconds by the presenter\n",
            "\n",
            "If the search term is 'Cameron' then the first search is to be done on the BBC site.\n",
            "\n",
            "If the search term is 'Cameron' thenNECT is to be used\n",
            "\n",
            "If the search term is 'Cameron' then the second search is to be done on the BBC site.\n",
            "\n",
            "If the search term is 'Cameron' thenS-Channel is to be used\n",
            "\n",
            "If the search term is 'Cameron' then the third search is to be done on the BBC site.\n",
            "\n",
            "If the search term is 'Cameron' thenE-Channel is to be used\n",
            "\n",
            "If the search term is 'Cameron' then the fourth search is to be done on the BBC site.\n",
            "\n",
            "S-Channel shows a new episode of the same series every Sunday evening for a period of seven days.\n",
            "\n",
            "E-Channel shows a new episode of the same series every day for a period of seven days.\n",
            "\n",
            "This is the final broadcast of the series, which is shown annually on the BBC's Channels.\n",
            "\n",
            "The new season is a programme of one of the three Channels, one of which is a live programme.\n",
            "\n",
            "The last episode of the series, if the search term is 'Cameron' and\n"
          ]
        }
      ],
      "source": [
        "print(conv_chain.run(\"Once upon a time in 1980 \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a275d61-9a9f-4226-bf30-0278f68b1ae0",
      "metadata": {
        "id": "2a275d61-9a9f-4226-bf30-0278f68b1ae0"
      },
      "source": [
        "#### Types of Memory\n",
        "\n",
        "1. ConversationBufferMemory: This memory allows for storing of messages and then extracts the messages in a\n",
        "variable\n",
        "2. ConversationBufferWindowMemory: keeps a list of the interactions of the conversation over time. It only uses the last K interactions. Useful for keeping a sliding window of the most recent interactions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
